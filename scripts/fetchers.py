#!/usr/bin/env python3
"""
æ•°æ®é‡‡é›†å™¨æ¨¡å—

æ”¯æŒå¤šç§æ•°æ®æºï¼š
- RSS è®¢é˜…æº
- HTML ç½‘é¡µçˆ¬è™«
- API æ¥å£
"""

import re
import json
from datetime import datetime, timezone
from typing import Optional
from urllib.parse import urljoin

import requests
import feedparser
from bs4 import BeautifulSoup


HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}


def fetch_rss_feed(feed_url: str, limit: int = 30) -> list[dict]:
    """æŠ“å– RSS æº"""
    try:
        feed = feedparser.parse(feed_url)
        items = []
        
        for entry in feed.entries[:limit]:
            summary = ""
            if hasattr(entry, "summary"):
                summary = entry.summary
            elif hasattr(entry, "description"):
                summary = entry.description
            elif hasattr(entry, "content") and entry.content:
                summary = entry.content[0].get("value", "")
            
            summary = re.sub(r"<[^>]+>", "", summary).strip()
            if len(summary) > 500:
                summary = summary[:500] + "..."
            
            published = None
            if hasattr(entry, "published_parsed") and entry.published_parsed:
                published = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
            elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
                published = datetime(*entry.updated_parsed[:6], tzinfo=timezone.utc)
            
            items.append({
                "title": entry.get("title", "Untitled"),
                "url": entry.get("link", ""),
                "summary": summary,
                "published": published.isoformat() if published else None,
            })
        
        return items
    
    except Exception as e:
        print(f"  [ERROR] Failed to fetch RSS {feed_url}: {e}")
        return []


def fetch_aibase_news(limit: int = 30) -> list[dict]:
    """çˆ¬å– AIbase æ–°é—»"""
    url = "https://www.aibase.com/zh/news"
    try:
        resp = requests.get(url, headers=HEADERS, timeout=30)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        
        items = []
        # AIbase ç»“æ„ï¼šé“¾æ¥åŒ…å« /news/
        article_links = soup.select("a[href*='/news/']")
        
        seen_urls = set()
        
        for link in article_links:
            href = link.get("href", "")
            if not href or href in seen_urls:
                continue
            
            # æ’é™¤éæ–°é—»è¯¦æƒ…é¡µ
            if not re.search(r"/news/\d+", href):
                continue
                
            full_url = urljoin(url, href)
            seen_urls.add(href)
            
            # è·å–æ ‡é¢˜å¹¶æ¸…ç†
            raw_title = link.get_text(strip=True)
            if not raw_title:
                continue
            
            # æ¸…ç† "åˆšåˆš.AIbase" ç­‰å‰ç¼€
            # é€šå¸¸æ ¼å¼æ˜¯ "æ—¶é—´.ä½œè€…æ ‡é¢˜"
            # æˆ‘ä»¬ç§»é™¤ .AIbase ä¹‹å‰çš„å†…å®¹
            title = re.sub(r'^.*\.AIbase', '', raw_title).strip()
            # å¦‚æœæ­£åˆ™æ²¡åŒ¹é…åˆ°ï¼ˆæ ¼å¼ä¸åŒï¼‰ï¼Œç›´æ¥ç”¨åŸæ ‡é¢˜
            if not title:
                title = raw_title
                
            # æ‘˜è¦ï¼šAIbase åˆ—è¡¨é¡µæ‘˜è¦æ˜¯ JS åŠ è½½çš„ ("åŠ è½½ä¸­...")
            # æˆ‘ä»¬ç›´æ¥ä½¿ç”¨æ ‡é¢˜ä½œä¸ºæ‘˜è¦ï¼Œæˆ–è€…è®© LLM åç»­è‡ªè¡Œç”Ÿæˆ
            summary = title 
            
            items.append({
                "title": title,
                "url": full_url,
                "summary": summary,
                "published": None,
            })
            
            if len(items) >= limit:
                break
        
        return items
    
    except Exception as e:
        print(f"  [ERROR] Failed to fetch AIbase: {e}")
        return []


def fetch_aibot_daily_news(limit: int = 30) -> list[dict]:
    """çˆ¬å– AIå·¥å…·é›† (ai-bot.cn/daily-ai-news/)"""
    url = "https://ai-bot.cn/daily-ai-news/"
    try:
        resp = requests.get(url, headers=HEADERS, timeout=30)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        
        items = []
        # æ ¹æ® Debug ç»“æœï¼šæ ‡é¢˜åœ¨ h2 ä¸­ï¼Œçˆ¶å®¹å™¨ class ä¸º news-content
        # æœ‰æ—¶å€™æ˜¯ h2 (å•æ¡æ–°é—»)ï¼Œæœ‰æ—¶å€™æ˜¯ h3
        news_items = soup.find_all(class_="news-content")
        
        if not news_items:
            # å¤‡ç”¨ï¼šæŸ¥æ‰¾æ‰€æœ‰ h2
            news_items = soup.find_all("h2")
            
        for container in news_items:
            # å¦‚æœ container æ˜¯ div.news-contentï¼Œæ‰¾é‡Œé¢çš„ h2
            if container.name == "div":
                title_elem = container.find(["h2", "h3"])
                # æ‰¾é“¾æ¥
                link_elem = container.find("a", href=True) or container.find_parent("a")
                # æ‰¾æ‘˜è¦
                desc_elem = container.find("p")
            else:
                # container æœ¬èº«å°±æ˜¯ h2
                title_elem = container
                link_elem = container.find("a", href=True)
                desc_elem = container.find_next("p")
            
            if not title_elem:
                continue
                
            title = title_elem.get_text(strip=True)
            if not title or len(title) < 5:
                continue
                
            # é“¾æ¥
            item_url = url
            if link_elem:
                item_url = link_elem["href"]
            
            # æ‘˜è¦
            summary = ""
            if desc_elem:
                summary = desc_elem.get_text(strip=True)
            if not summary:
                summary = title
                
            items.append({
                "title": title,
                "url": item_url,
                "summary": summary,
                "published": None,
            })
            
            if len(items) >= limit:
                break
        
        return items
    
    except Exception as e:
        print(f"  [ERROR] Failed to fetch AIå·¥å…·é›†: {e}")
        return []


def fetch_ithome_ai_news(limit: int = 30) -> list[dict]:
    """æŠ“å– ITä¹‹å®¶ AI æ ‡ç­¾é¡µ (æ›¿ä»£ RSS è¿‡æ»¤)"""
    url = "https://www.ithome.com/tag/ai"
    try:
        resp = requests.get(url, headers=HEADERS, timeout=30)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")
        
        items = []
        # åˆ—è¡¨é¡¹é€šå¸¸åœ¨ .run_list li æˆ– .news-list li
        list_items = soup.select(".block li, .news-list li, ul.bl li")
        
        seen_urls = set()
        
        for li in list_items:
            # æŸ¥æ‰¾é“¾æ¥
            a_tag = li.find("a", href=True)
            if not a_tag:
                continue
                
            href = a_tag["href"]
            if href in seen_urls:
                continue
            
            full_url = href # ITä¹‹å®¶é€šå¸¸æ˜¯å®Œæ•´é“¾æ¥
            if not full_url.startswith("http"):
                full_url = urljoin("https://www.ithome.com", href)
                
            seen_urls.add(href)
            
            # æ ‡é¢˜
            title = a_tag.get_text(strip=True)
            # æœ‰æ—¶å€™æ ‡é¢˜åœ¨ h2 æˆ– inside div
            if not title:
                title_elem = li.find(["h2", "h3", ".title"])
                if title_elem:
                    title = title_elem.get_text(strip=True)
            
            if not title:
                continue
                
            # æ‘˜è¦
            summary = ""
            desc_elem = li.find(class_="memo") or li.find(class_="m")
            if desc_elem:
                summary = desc_elem.get_text(strip=True)
            
            # æ—¶é—´
            published = None
            date_elem = li.find(class_="time") or li.find(class_="t")
            # å¤„ç†æ—¶é—´å­—ç¬¦ä¸²... è¿™é‡Œç®€åŒ–ï¼Œç”±åç»­æµç¨‹å¤„ç†
            
            items.append({
                "title": title,
                "url": full_url,
                "summary": summary,
                "published": None,
            })
            
            if len(items) >= limit:
                break
        
        return items
    
    except Exception as e:
        print(f"  [ERROR] Failed to fetch ITä¹‹å®¶: {e}")
        return []


def fetch_github_trending_ai(limit: int = 30) -> list[dict]:
    """é€šè¿‡ GitHub Search API è·å– AI ç›¸å…³çƒ­é—¨ä»“åº“"""
    api_url = "https://api.github.com/search/repositories"
    params = {
        "q": "topic:machine-learning stars:>1000",
        "sort": "updated",
        "order": "desc",
        "per_page": min(limit, 100)
    }
    
    try:
        resp = requests.get(api_url, params=params, headers=HEADERS, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        
        items = []
        for repo in data.get("items", [])[:limit]:
            items.append({
                "name": repo["full_name"],
                "url": repo["html_url"],
                "description": repo.get("description") or "",
                "stars": repo["stargazers_count"],
                "forks": repo["forks_count"],
                "language": repo.get("language") or "",
                "topics": repo.get("topics", []),
                "updated_at": repo.get("pushed_at"),
            })
        
        return items
    
    except Exception as e:
        print(f"  [ERROR] Failed to fetch GitHub Trending: {e}")
        return []


def fetch_huggingface_trending(limit: int = 30) -> list[dict]:
    """é€šè¿‡ HuggingFace API è·å–çƒ­é—¨æ¨¡å‹"""
    api_url = "https://huggingface.co/api/models"
    params = {
        "sort": "trendingScore",
        "direction": "-1",
        "limit": min(limit, 100)
    }
    
    try:
        resp = requests.get(api_url, params=params, headers=HEADERS, timeout=30)
        resp.raise_for_status()
        models = resp.json()
        
        items = []
        for model in models[:limit]:
            model_id = model.get("id") or model.get("modelId", "")
            items.append({
                "model_id": model_id,
                "url": f"https://huggingface.co/{model_id}",
                "likes": model.get("likes", 0),
                "downloads": model.get("downloads", 0),
                "trending_score": model.get("trendingScore", 0),
                "pipeline_tag": model.get("pipeline_tag") or "",
                "tags": model.get("tags", []),
                "created_at": model.get("createdAt"),
            })
        
        return items
    
    except Exception as e:
        print(f"  [ERROR] Failed to fetch HuggingFace Trending: {e}")
        return []


if __name__ == "__main__":
    print("Testing fetchers...\n")
    
    print("1. AIbase News:")
    items = fetch_aibase_news(5)
    for i, item in enumerate(items, 1):
        print(f"   {i}. {item['title'][:50]}...")
    print(f"   Total: {len(items)}\n")
    
    print("2. AIå·¥å…·é›† Daily News:")
    items = fetch_aibot_daily_news(5)
    for i, item in enumerate(items, 1):
        print(f"   {i}. {item['title'][:50]}...")
    print(f"   Total: {len(items)}\n")
    
    print("3. ITä¹‹å®¶ AI News:")
    items = fetch_ithome_ai_news(5)
    for i, item in enumerate(items, 1):
        print(f"   {i}. {item['title'][:50]}...")
    print(f"   Total: {len(items)}\n")
    
    print("4. GitHub Trending AI:")
    items = fetch_github_trending_ai(5)
    for i, item in enumerate(items, 1):
        print(f"   {i}. {item['name']} â­{item['stars']}")
    print(f"   Total: {len(items)}\n")
    
    print("5. HuggingFace Trending:")
    items = fetch_huggingface_trending(5)
    for i, item in enumerate(items, 1):
        print(f"   {i}. {item['model_id']} ğŸ”¥{item['trending_score']}")
    print(f"   Total: {len(items)}\n")
