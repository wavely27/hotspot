#!/usr/bin/env python3
"""
Daily AI Hotspot Fetcher

ä»å¤šä¸ª RSS æºæŠ“å–å†…å®¹ï¼Œé€šè¿‡ Gemini ç­›é€‰ AI ç›¸å…³çƒ­ç‚¹ï¼Œ
å­˜å…¥ Supabase æ•°æ®åº“ï¼Œå¹¶ç”Ÿæˆæ¯æ—¥æŠ¥å‘Šã€‚
"""

import json
import os
import re
import sys
from datetime import datetime, timezone
from pathlib import Path

from dotenv import load_dotenv
load_dotenv(Path(__file__).parent.parent / ".env")

import feedparser
from openai import OpenAI
from supabase import create_client, Client

from fetchers import (
    fetch_rss_feed,
    fetch_aibase_news,
    fetch_aibot_daily_news,
    fetch_ithome_ai_news,
    fetch_github_trending_ai,
    fetch_huggingface_trending,
)

# ============================================================================
# é…ç½®
# ============================================================================

# æ¯ä¸ªæºæœ€å¤šä¿ç•™çš„æ¡ç›®æ•°
MAX_ITEMS_PER_SOURCE = 10

# æ¯ä¸ªæºæŠ“å–çš„åŸå§‹æ¡ç›®æ•°ï¼ˆç”¨äºç­›é€‰ï¼‰
FETCH_ITEMS_PER_SOURCE = 30

# MegaLLM æ¨¡å‹
LLM_MODEL = "moonshotai/kimi-k2-instruct-0905"

# ============================================================================
# åˆå§‹åŒ–
# ============================================================================

def init_llm() -> OpenAI:
    """åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ (MegaLLM)"""
    api_key = os.environ.get("MEGALLM_API_KEY")
    if not api_key:
        raise ValueError("MEGALLM_API_KEY environment variable is required")
    
    return OpenAI(
        base_url="https://ai.megallm.io/v1",
        api_key=api_key
    )


def init_supabase() -> Client:
    """åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯"""
    url = os.environ.get("SUPABASE_URL")
    key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")
    
    if not url or not key:
        raise ValueError("SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY are required")
    
    return create_client(url, key)


def load_feed_config() -> tuple[list[dict], dict]:
    """åŠ è½½æ•°æ®æºé…ç½®ï¼Œè¿”å› (feeds, trending)"""
    config_path = Path(__file__).parent.parent / "config" / "info_map.json"
    with open(config_path, "r", encoding="utf-8") as f:
        config = json.load(f)
    return config.get("feeds", []), config.get("trending", {})


# ============================================================================
# æ•°æ®æŠ“å–ï¼ˆRSS + çˆ¬è™«ï¼‰
# ============================================================================

# çˆ¬è™«å‡½æ•°æ˜ å°„
CRAWLER_MAP = {
    "fetch_aibase_news": fetch_aibase_news,
    "fetch_aibot_daily_news": fetch_aibot_daily_news,
    "fetch_ithome_ai_news": fetch_ithome_ai_news,
}


def fetch_all_feeds(feeds: list[dict]) -> dict[str, list[dict]]:
    """
    æŠ“å–æ‰€æœ‰æ•°æ®æºï¼ˆæ”¯æŒ RSS å’Œ çˆ¬è™«ï¼‰
    
    Returns:
        dict mapping source name to list of items
    """
    all_items = {}
    
    for feed in feeds:
        name = feed["name"]
        feed_type = feed.get("type", "rss")
        print(f"Fetching: {name} ({feed_type})...")
        
        items = []
        try:
            if feed_type == "rss":
                url = feed.get("url", "")
                if url:
                    items = fetch_rss_feed(url, limit=FETCH_ITEMS_PER_SOURCE)
            elif feed_type == "crawler":
                fetcher_name = feed.get("fetcher", "")
                fetcher_func = CRAWLER_MAP.get(fetcher_name)
                if fetcher_func:
                    items = fetcher_func(limit=FETCH_ITEMS_PER_SOURCE)
                else:
                    print(f"  [WARN] Unknown fetcher: {fetcher_name}")
        except Exception as e:
            print(f"  [ERROR] Failed to fetch {name}: {e}")
        
        if items:
            all_items[name] = items
            print(f"  Found {len(items)} items")
        else:
            print(f"  No items found")
    
    return all_items


# ============================================================================
# Gemini ç­›é€‰
# ============================================================================

FILTER_PROMPT = """ä½ æ˜¯ä¸€ä¸ª AI æŠ€æœ¯çƒ­ç‚¹ç­›é€‰åŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹æ–‡ç« åˆ—è¡¨ä¸­ï¼Œç­›é€‰å‡ºä¸ AI/äººå·¥æ™ºèƒ½æœ€ç›¸å…³ã€æœ€æœ‰ä»·å€¼çš„å†…å®¹ã€‚

è¯„åˆ¤æ ‡å‡†ï¼š
1. ä¸ AIã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€å¤§è¯­è¨€æ¨¡å‹ç­‰ç›´æ¥ç›¸å…³
2. å†…å®¹æœ‰ä»·å€¼ï¼ˆæŠ€æœ¯çªç ´ã€é‡è¦å‘å¸ƒã€è¡Œä¸šåŠ¨æ€ç­‰ï¼‰
3. ä¼˜å…ˆé€‰æ‹©çƒ­åº¦é«˜ã€å½±å“åŠ›å¤§çš„å†…å®¹

è¯·ä»ä¸‹é¢çš„æ–‡ç« ä¸­é€‰å‡ºæœ€å¤š {limit} ç¯‡æœ€ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ã€‚

æ–‡ç« åˆ—è¡¨ï¼š
{articles}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼Œ**æ‰€æœ‰æ–‡æœ¬å†…å®¹å¿…é¡»ç¿»è¯‘æˆä¸­æ–‡**ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```json
{{
  "selected": [
    {{
      "index": 0,
      "title_cn": "ä¸­æ–‡æ ‡é¢˜",
      "reason_cn": "ä¸­æ–‡æ¨èç†ç”±ï¼ˆä½œä¸ºè¯¥èµ„è®¯çš„ç®€çŸ­æ€»ç»“ï¼Œ50å­—ä»¥å†…ï¼‰"
    }}
  ]
}}
```

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚index æ˜¯æ–‡ç« åœ¨åˆ—è¡¨ä¸­çš„ç´¢å¼•ï¼ˆä» 0 å¼€å§‹ï¼‰ã€‚
"""

SUMMARY_PROMPT = """è¯·æ ¹æ®ä»¥ä¸‹ä»Šæ—¥ AI çƒ­ç‚¹æ–°é—»çš„æ ‡é¢˜å’Œæ¨èç†ç”±ï¼Œå†™ä¸€æ®µçº¦ 50 å­—çš„ç®€çŸ­æ—¥æŠ¥ç»¼è¿°ã€‚
ç»¼è¿°åº”ç‚¹å‡ºä»Šå¤©æœ€å€¼å¾—å…³æ³¨çš„ 1-3 ä¸ªæ ¸å¿ƒçƒ­ç‚¹äº‹ä»¶ã€‚

çƒ­ç‚¹åˆ—è¡¨ï¼š
{content}

è¯·ç›´æ¥è¿”å›ç»¼è¿°æ–‡æœ¬ï¼Œä¸è¦åŠ ä»»ä½•å‰ç¼€æˆ–æ ¼å¼ã€‚
"""

GITHUB_TRANSLATE_PROMPT = """è¯·ä¸ºä»¥ä¸‹ GitHub AI é¡¹ç›®ç”Ÿæˆä¸­æ–‡ä»‹ç»ã€‚

é¡¹ç›®åˆ—è¡¨ï¼š
{projects}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼š
```json
{{
  "translated": [
    {{
      "index": 0,
      "description_cn": "é¡¹ç›®ä¸­æ–‡ä»‹ç»ï¼ˆä¸€å¥è¯ï¼Œ50å­—ä»¥å†…ï¼‰",
      "ai_reason": "æ¨èç†ç”±ï¼ˆä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨ï¼Œ30å­—ä»¥å†…ï¼‰"
    }}
  ]
}}
```

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""

HUGGINGFACE_TRANSLATE_PROMPT = """è¯·ä¸ºä»¥ä¸‹ HuggingFace çƒ­é—¨æ¨¡å‹ç”Ÿæˆä¸­æ–‡ä»‹ç»ã€‚

æ¨¡å‹åˆ—è¡¨ï¼š
{models}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼š
```json
{{
  "translated": [
    {{
      "index": 0,
      "description_cn": "æ¨¡å‹ä¸­æ–‡ä»‹ç»ï¼ˆä¸€å¥è¯ï¼Œ50å­—ä»¥å†…ï¼‰",
      "ai_reason": "æ¨èç†ç”±ï¼ˆä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨ï¼Œ30å­—ä»¥å†…ï¼‰"
    }}
  ]
}}
```

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""


def filter_items_with_gemini(
    client: OpenAI,
    items: list[dict],
    limit: int = MAX_ITEMS_PER_SOURCE
) -> list[dict]:
    """
    ä½¿ç”¨ LLM ç­›é€‰æœ€ç›¸å…³çš„æ–‡ç« ï¼Œå¹¶ç”Ÿæˆä¸­æ–‡æ ‡é¢˜å’Œç†ç”±
    """
    if not items:
        return []
    
    # æ„å»ºæ–‡ç« åˆ—è¡¨æ–‡æœ¬
    articles_text = ""
    for i, item in enumerate(items):
        articles_text += f"\n[{i}] æ ‡é¢˜: {item['title']}\n"
        if item.get("summary"):
            articles_text += f"    æ‘˜è¦: {item['summary'][:200]}\n"
    
    prompt = FILTER_PROMPT.format(limit=limit, articles=articles_text)
    
    try:
        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        response_text = (response.choices[0].message.content or "").strip()
        
        # æå– JSON (å¦‚æœ response_format ä¸ç”Ÿæ•ˆï¼Œæ‰‹åŠ¨æå–)
        json_match = re.search(r"\{[\s\S]*\}", response_text)
        if not json_match:
            print("  [WARN] Could not parse LLM response, returning original items")
            return items[:limit]
        
        result = json.loads(json_match.group())
        
        selected_map = {item["index"]: item for item in result.get("selected", [])}
        selected_indices = [item["index"] for item in result.get("selected", [])]
        
        # æ ¹æ®ç´¢å¼•æå–æ–‡ç« å¹¶æ›´æ–°ä¸ºä¸­æ–‡ä¿¡æ¯
        filtered = []
        for idx in selected_indices:
            if 0 <= idx < len(items):
                item = items[idx].copy()
                gemini_data = selected_map.get(idx, {})
                
                # ä½¿ç”¨ä¸­æ–‡ä¿¡æ¯è¦†ç›–æˆ–æ–°å¢å­—æ®µ
                if gemini_data.get("title_cn"):
                    item["title"] = gemini_data["title_cn"]
                if gemini_data.get("reason_cn"):
                    item["ai_reason"] = gemini_data["reason_cn"]
                
                filtered.append(item)
        
        return filtered[:limit]
    
    except Exception as e:
        print(f"  [ERROR] LLM filtering failed: {e}")
        # å‡ºé”™æ—¶é™çº§ï¼šè¿”å›åŸæ–‡å‰ N æ¡
        return items[:limit]


def generate_daily_summary(client: OpenAI, all_selected: dict[str, list[dict]]) -> str:
    """ç”Ÿæˆæ—¥æŠ¥æ•´ä½“ç»¼è¿°"""
    content = ""
    for source, items in all_selected.items():
        for item in items:
            title = item.get("title", "")
            reason = item.get("ai_reason", "")
            content += f"- {title}: {reason}\n"
    
    if not content:
        return "ä»Šæ—¥æš‚æ— é‡ç‚¹èµ„è®¯ã€‚"

    prompt = SUMMARY_PROMPT.format(content=content[:5000]) # é™åˆ¶é•¿åº¦
    
    try:
        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": prompt}]
        )
        return (response.choices[0].message.content or "").strip() or "ä»Šæ—¥ AI çƒ­ç‚¹èµ„è®¯æ±‡æ€»ã€‚"
    except Exception as e:
        print(f"  [WARN] Failed to generate summary: {e}")
        return "ä»Šæ—¥ AI çƒ­ç‚¹èµ„è®¯æ±‡æ€»ã€‚"


def translate_github_trending(client: OpenAI, items: list[dict], limit: int = 20) -> list[dict]:
    if not items:
        return []
    
    items = items[:limit]
    projects_text = ""
    for i, item in enumerate(items):
        projects_text += f"\n[{i}] {item['name']}"
        projects_text += f"\n    â­{item['stars']} | Language: {item.get('language', 'N/A')}"
        if item.get("description"):
            projects_text += f"\n    {item['description'][:200]}"
    
    prompt = GITHUB_TRANSLATE_PROMPT.format(projects=projects_text)
    
    try:
        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        response_text = (response.choices[0].message.content or "").strip()
        
        json_match = re.search(r"\{[\s\S]*\}", response_text)
        if not json_match:
            return items
        
        result = json.loads(json_match.group())
        translated_map = {t["index"]: t for t in result.get("translated", [])}
        
        for i, item in enumerate(items):
            if i in translated_map:
                t = translated_map[i]
                item["description_cn"] = t.get("description_cn", "")
                item["ai_reason"] = t.get("ai_reason", "")
        
        return items
    except Exception as e:
        print(f"  [ERROR] GitHub translate failed: {e}")
        return items


def translate_huggingface_trending(client: OpenAI, items: list[dict], limit: int = 20) -> list[dict]:
    if not items:
        return []
    
    items = items[:limit]
    models_text = ""
    for i, item in enumerate(items):
        models_text += f"\n[{i}] {item['model_id']}"
        models_text += f"\n    ğŸ”¥{item.get('trending_score', 0)} | Task: {item.get('pipeline_tag', 'N/A')}"
        models_text += f"\n    Downloads: {item.get('downloads', 0)} | Likes: {item.get('likes', 0)}"
    
    prompt = HUGGINGFACE_TRANSLATE_PROMPT.format(models=models_text)
    
    try:
        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        response_text = (response.choices[0].message.content or "").strip()
        
        json_match = re.search(r"\{[\s\S]*\}", response_text)
        if not json_match:
            return items
        
        result = json.loads(json_match.group())
        translated_map = {t["index"]: t for t in result.get("translated", [])}
        
        for i, item in enumerate(items):
            if i in translated_map:
                t = translated_map[i]
                item["description_cn"] = t.get("description_cn", "")
                item["ai_reason"] = t.get("ai_reason", "")
        
        return items
    except Exception as e:
        print(f"  [ERROR] HuggingFace translate failed: {e}")
        return items


# ============================================================================
# æ•°æ®åº“æ“ä½œ
# ============================================================================

def ensure_tables_exist(supabase: Client) -> None:
    """ç¡®ä¿æ‰€éœ€çš„è¡¨å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡æ—¥æŠ¥ä¿å­˜"""
    try:
        # æ£€æŸ¥ daily_reports è¡¨æ˜¯å¦å­˜åœ¨
        supabase.table("daily_reports").select("id").limit(1).execute()
        # print("  daily_reports table: OK")
    except Exception:
        print("  [WARN] daily_reports table not found. Daily report will not be saved.")
        print("  To create it, run this SQL in Supabase Dashboard:")
        print("  CREATE TABLE daily_reports (")
        print("    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,")
        print("    report_date DATE DEFAULT CURRENT_DATE UNIQUE,")
        print("    content TEXT NOT NULL,")
        print("    summary TEXT,")
        print("    created_at TIMESTAMPTZ DEFAULT NOW()")
        print("  );")


def upsert_hotspots(supabase: Client, items: list[dict], source: str) -> int:
    """
    å°†çƒ­ç‚¹æ•°æ®å†™å…¥ Supabase
    """
    if not items:
        return 0
    
    records = []
    for item in items:
        records.append({
            "title": item["title"],
            "url": item["url"],
            "summary": item.get("ai_reason") or item.get("summary", ""), # ä¼˜å…ˆä½¿ç”¨ AI ç”Ÿæˆçš„ä¸­æ–‡ç†ç”±
            "source": source,
            "is_published": True,
        })
    
    try:
        # ä½¿ç”¨ upsertï¼Œä»¥ url ä¸ºå†²çªæ£€æµ‹å­—æ®µ
        result = supabase.table("hotspots").upsert(
            records,
            on_conflict="url"
        ).execute()
        
        return len(result.data) if result.data else 0
    
    except Exception as e:
        print(f"  [ERROR] Failed to upsert hotspots: {e}")
        return 0


def save_daily_report(supabase: Client, report_content: str, summary: str = "") -> bool:
    """ä¿å­˜æ¯æ—¥æŠ¥å‘Šåˆ°æ•°æ®åº“ï¼ˆå¢é‡æ¨¡å¼ï¼šè¿½åŠ æ–°æ¥æºï¼Œä¸è¦†ç›–å·²æœ‰å†…å®¹ï¼‰"""
    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    
    try:
        # 1. æŸ¥è¯¢å½“å¤©æ˜¯å¦å·²æœ‰æ—¥æŠ¥
        existing = supabase.table("daily_reports").select("content, summary").eq("report_date", today).execute()
        
        if existing.data:
            # 2. å·²æœ‰æ—¥æŠ¥ï¼šæ™ºèƒ½åˆå¹¶
            old_content = existing.data[0].get("content", "")
            old_summary = existing.data[0].get("summary", "")
            
            # è§£æå·²æœ‰çš„æ¥æºï¼ˆ## å¼€å¤´çš„è¡Œï¼‰
            existing_sources = set(re.findall(r"^## (.+)$", old_content, re.MULTILINE))
            
            # è§£ææ–°å†…å®¹ä¸­çš„æ¥æºå’Œå¯¹åº”å†…å®¹å—
            new_sections = re.split(r"(?=^## )", report_content, flags=re.MULTILINE)
            
            # ç­›é€‰å‡ºæ–°æ¥æºçš„å†…å®¹å—
            new_content_parts = []
            for section in new_sections:
                match = re.match(r"^## (.+)$", section, re.MULTILINE)
                if match:
                    source_name = match.group(1)
                    if source_name not in existing_sources:
                        new_content_parts.append(section)
            
            if new_content_parts:
                # è¿½åŠ æ–°æ¥æºåˆ°å·²æœ‰å†…å®¹æœ«å°¾
                merged_content = old_content.rstrip() + "\n\n" + "\n".join(new_content_parts)
                # æ›´æ–° summaryï¼ˆå¯é€‰ï¼šè¿½åŠ æˆ–ä¿æŒåŸæœ‰ï¼‰
                merged_summary = old_summary if old_summary else summary
            else:
                # æ²¡æœ‰æ–°æ¥æºï¼Œä¿æŒåŸæ ·
                merged_content = old_content
                merged_summary = old_summary
            
            data = {
                "report_date": today,
                "content": merged_content,
                "summary": merged_summary
            }
        else:
            # 3. æ²¡æœ‰å·²æœ‰æ—¥æŠ¥ï¼šç›´æ¥æ’å…¥
            data = {
                "report_date": today,
                "content": report_content,
                "summary": summary
            }
        
        supabase.table("daily_reports").upsert(data, on_conflict="report_date").execute()
        return True
    except Exception as e:
        print(f"[ERROR] Failed to save daily report: {e}")
        return False


def upsert_github_trending(supabase: Client, items: list[dict]) -> int:
    if not items:
        return 0
    
    records = []
    for item in items:
        records.append({
            "name": item["name"],
            "url": item["url"],
            "description": item.get("description", ""),
            "description_cn": item.get("description_cn", ""),
            "stars": item.get("stars", 0),
            "forks": item.get("forks", 0),
            "language": item.get("language", ""),
            "topics": item.get("topics", []),
            "ai_reason": item.get("ai_reason", ""),
            "is_published": True,
        })
    
    try:
        result = supabase.table("github_trending").upsert(
            records, on_conflict="url"
        ).execute()
        return len(result.data) if result.data else 0
    except Exception as e:
        print(f"  [ERROR] Failed to upsert github_trending: {e}")
        return 0


def upsert_huggingface_trending(supabase: Client, items: list[dict]) -> int:
    if not items:
        return 0
    
    records = []
    for item in items:
        records.append({
            "model_id": item["model_id"],
            "url": item["url"],
            "description_cn": item.get("description_cn", ""),
            "likes": item.get("likes", 0),
            "downloads": item.get("downloads", 0),
            "trending_score": item.get("trending_score", 0),
            "pipeline_tag": item.get("pipeline_tag", ""),
            "tags": item.get("tags", []),
            "ai_reason": item.get("ai_reason", ""),
            "is_published": True,
        })
    
    try:
        result = supabase.table("huggingface_trending").upsert(
            records, on_conflict="url"
        ).execute()
        return len(result.data) if result.data else 0
    except Exception as e:
        print(f"  [ERROR] Failed to upsert huggingface_trending: {e}")
        return 0


# ============================================================================
# æŠ¥å‘Šç”Ÿæˆ
# ============================================================================

def generate_daily_report(
    all_selected: dict[str, list[dict]], 
    daily_summary: str = ""
) -> str:
    """ç”Ÿæˆæ¯æ—¥ Markdown æŠ¥å‘Šï¼ˆä¸­æ–‡ç‰ˆï¼‰"""
    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    
    lines = [
        f"# AI çƒ­ç‚¹æ—¥æŠ¥ - {today}",
        "",
        f"ç”Ÿæˆæ—¶é—´: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')} UTC",
        "",
    ]
    
    if daily_summary:
        lines.append(f"> **ä»Šæ—¥ç»¼è¿°**ï¼š{daily_summary}")
        lines.append("")
        lines.append("---")
        lines.append("")
    
    total_count = sum(len(items) for items in all_selected.values())
    lines.append(f"ä»Šæ—¥å…±æ”¶å½• **{total_count}** æ¡çƒ­ç‚¹ï¼Œæ¥è‡ª **{len(all_selected)}** ä¸ªä¿¡æ¯æºã€‚")
    lines.append("")
    
    for source, items in all_selected.items():
        lines.append(f"## {source}")
        lines.append("")
        
        for i, item in enumerate(items):
            title = item["title"] # å·²ç»æ˜¯ä¸­æ–‡
            url = item["url"]
            # ä¼˜å…ˆä½¿ç”¨ AI ç”Ÿæˆçš„ç†ç”±ï¼Œå¦åˆ™ä½¿ç”¨åŸæ–‡æ‘˜è¦
            reason = item.get("ai_reason") or item.get("summary", "")
            # é™åˆ¶æ‘˜è¦é•¿åº¦ï¼Œé¿å…è¿‡é•¿
            if reason and len(reason) > 200:
                reason = reason[:200] + "..."
            
            lines.append(f"### {i+1}. [{title}]({url})")
            if reason:
                lines.append(f"> {reason}")
            lines.append("")
        
        lines.append("")
    
    return "\n".join(lines)


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    print("=" * 60)
    print("AI Hotspot Daily Fetcher")
    print("=" * 60)
    print()
    
    print("[1/8] Initializing...")
    try:
        client = init_llm()
        supabase = init_supabase()
        feeds, trending_config = load_feed_config()
        print(f"  Loaded {len(feeds)} feed sources + {len(trending_config)} trending sources")
        ensure_tables_exist(supabase)
    except Exception as e:
        print(f"[FATAL] Initialization failed: {e}")
        sys.exit(1)
    
    print()
    print("[2/8] Fetching feeds (RSS + crawlers)...")
    raw_data = fetch_all_feeds(feeds)
    
    if not raw_data:
        print("[WARN] No data fetched from any source")
    
    print()
    print("[3/8] Filtering & Translating feeds with LLM...")
    all_selected = {}
    
    for source, items in raw_data.items():
        print(f"  Processing: {source} ({len(items)} items)")
        selected = filter_items_with_gemini(client, items)
        if selected:
            all_selected[source] = selected
            print(f"    Selected {len(selected)} items")
    
    print()
    print("[4/8] Fetching trending data...")
    github_items = []
    huggingface_items = []
    
    if "github" in trending_config:
        print("  Fetching GitHub Trending AI...")
        github_items = fetch_github_trending_ai(limit=30)
        print(f"    Found {len(github_items)} repos")
    
    if "huggingface" in trending_config:
        print("  Fetching HuggingFace Trending...")
        huggingface_items = fetch_huggingface_trending(limit=30)
        print(f"    Found {len(huggingface_items)} models")
    
    print()
    print("[5/8] Translating trending data with LLM...")
    if github_items:
        print("  Translating GitHub items...")
        github_items = translate_github_trending(client, github_items, limit=20)
    if huggingface_items:
        print("  Translating HuggingFace items...")
        huggingface_items = translate_huggingface_trending(client, huggingface_items, limit=20)
    
    print()
    print("[6/8] Generating daily summary...")
    daily_summary = ""
    if all_selected:
        try:
            daily_summary = generate_daily_summary(client, all_selected)
            print(f"  Summary: {daily_summary}")
        except Exception as e:
            print(f"  [WARN] Skipped summary generation: {e}")
    
    print()
    print("[7/8] Saving to database...")
    total_saved = 0
    
    for source, items in all_selected.items():
        count = upsert_hotspots(supabase, items, source)
        total_saved += count
        print(f"  {source}: {count} records")
    
    if github_items:
        count = upsert_github_trending(supabase, github_items)
        total_saved += count
        print(f"  GitHub Trending: {count} records")
    
    if huggingface_items:
        count = upsert_huggingface_trending(supabase, huggingface_items)
        total_saved += count
        print(f"  HuggingFace Trending: {count} records")
    
    print(f"  Total saved: {total_saved} records")
    
    print()
    print("[8/8] Generating & Saving daily report...")
    report = generate_daily_report(all_selected, daily_summary)
    
    if save_daily_report(supabase, report, daily_summary):
        print("  Daily report saved successfully")
    else:
        print("  [WARN] Failed to save daily report")
    
    print()
    print("=" * 60)
    print("REPORT PREVIEW")
    print("=" * 60)
    print(report[:2000])
    if len(report) > 2000:
        print("... (truncated)")
    
    print()
    print("=" * 60)
    print("COMPLETED")
    print("=" * 60)


if __name__ == "__main__":
    main()
