#!/usr/bin/env python3
"""
Daily AI Hotspot Fetcher

ä»å¤šä¸ª RSS æºæŠ“å–å†…å®¹ï¼Œé€šè¿‡ Gemini ç­›é€‰ AI ç›¸å…³çƒ­ç‚¹ï¼Œ
å­˜å…¥ Supabase æ•°æ®åº“ï¼Œå¹¶ç”Ÿæˆæ¯æ—¥æŠ¥å‘Šã€‚
"""

import re
import sys
import json
import os
import concurrent.futures
import time
import difflib
import uuid
from typing import Any, cast
from datetime import datetime, timezone
from pathlib import Path

from dotenv import load_dotenv
load_dotenv(Path(__file__).parent.parent / ".env")

import feedparser
from openai import OpenAI
from supabase import create_client, Client

from fetchers import (
    fetch_rss_feed,
    fetch_aibase_news,
    fetch_aibot_daily_news,
    fetch_ithome_ai_news,
    fetch_github_trending_ai,
    fetch_huggingface_trending,
)

# ============================================================================
# é…ç½®
# ============================================================================

# æ¯ä¸ªæºæœ€å¤šä¿ç•™çš„æ¡ç›®æ•°
MAX_ITEMS_PER_SOURCE = int(os.environ.get("MAX_ITEMS_PER_SOURCE", 10))

# æ¯ä¸ªæºæŠ“å–çš„åŸå§‹æ¡ç›®æ•°ï¼ˆç”¨äºç­›é€‰ï¼‰
FETCH_ITEMS_PER_SOURCE = int(os.environ.get("FETCH_ITEMS_PER_SOURCE", 30))

LLM_MODELS = [
    "deepseek-ai/deepseek-v3.1",
    "deepseek-ai/deepseek-v3.1-terminus",
    "qwen/qwen3-next-80b-a3b-instruct",
]

# ============================================================================
# åˆå§‹åŒ–
# ============================================================================

def init_llm() -> OpenAI:
    """åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ (MegaLLM)"""
    api_key = os.environ.get("MEGALLM_API_KEY")
    if not api_key:
        raise ValueError("MEGALLM_API_KEY environment variable is required")
    
    return OpenAI(
        base_url="https://ai.megallm.io/v1",
        api_key=api_key
    )


def init_supabase() -> Client:
    """åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯"""
    url = os.environ.get("SUPABASE_URL")
    key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")
    
    if not url or not key:
        raise ValueError("SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY are required")
    
    return create_client(url, key)


def load_feed_config() -> tuple[list[dict], dict]:
    """åŠ è½½æ•°æ®æºé…ç½®ï¼Œè¿”å› (feeds, trending)"""
    config_path = Path(__file__).parent.parent / "config" / "info_map.json"
    with open(config_path, "r", encoding="utf-8") as f:
        config = json.load(f)
    return config.get("feeds", []), config.get("trending", {})


# ============================================================================
# æ•°æ®æŠ“å–ï¼ˆRSS + çˆ¬è™«ï¼‰
# ============================================================================

# çˆ¬è™«å‡½æ•°æ˜ å°„
CRAWLER_MAP = {
    "fetch_aibase_news": fetch_aibase_news,
    "fetch_aibot_daily_news": fetch_aibot_daily_news,
    "fetch_ithome_ai_news": fetch_ithome_ai_news,
}


def fetch_all_feeds(feeds: list[dict]) -> dict[str, list[dict]]:
    """
    æŠ“å–æ‰€æœ‰æ•°æ®æºï¼ˆæ”¯æŒ RSS å’Œ çˆ¬è™«ï¼‰
    
    Returns:
        dict mapping source name to list of items
    """
    all_items = {}
    
    def fetch_with_timeout(name: str, feed_type: str, feed_config: dict) -> list[dict]:
        """Fetch a single feed with timeout handling"""
        print(f"Fetching: {name} ({feed_type})...")
        items = []
        try:
            if feed_type == "rss":
                url = feed_config.get("url", "")
                if url:
                    # Feedparser handles timeouts internally or we can wrap it
                    items = fetch_rss_feed(url, limit=FETCH_ITEMS_PER_SOURCE)
            elif feed_type == "crawler":
                fetcher_name = feed_config.get("fetcher", "")
                fetcher_func = CRAWLER_MAP.get(fetcher_name)
                if fetcher_func:
                    items = fetcher_func(limit=FETCH_ITEMS_PER_SOURCE)
                else:
                    print(f"  [WARN] Unknown fetcher: {fetcher_name}")
        except Exception as e:
            print(f"  [ERROR] Failed to fetch {name}: {e}")
            
        if items:
            print(f"  Found {len(items)} items from {name}")
            return items
        print(f"  No items found from {name}")
        return []

    # Use ThreadPoolExecutor for parallel fetching
    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
        future_to_feed = {}
        for feed in feeds:
            name = feed["name"]
            feed_type = feed.get("type", "rss")
            future = executor.submit(fetch_with_timeout, name, feed_type, feed)
            future_to_feed[future] = name

        for future in concurrent.futures.as_completed(future_to_feed):
            name = future_to_feed[future]
            try:
                items = future.result()
                if items:
                    all_items[name] = items
            except Exception as e:
                print(f"  [ERROR] Exception fetching {name}: {e}")

    return all_items


# ============================================================================
# LLM API è°ƒç”¨ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
# ============================================================================

MAX_RETRIES = 3
RETRY_BASE_DELAY = 5

def call_llm_with_retry(
    client: OpenAI,
    messages: list[dict],
    response_format: dict | None = None,
) -> str | None:
    for model in LLM_MODELS:
        for attempt in range(MAX_RETRIES):
            try:
                kwargs: dict[str, Any] = {
                    "model": model,
                    "messages": messages,
                }
                if response_format:
                    kwargs["response_format"] = response_format
                
                response = client.chat.completions.create(**kwargs)
                return (response.choices[0].message.content or "").strip()
            except Exception as e:
                error_msg = str(e).lower()
                is_rate_limit = "rate_limit" in error_msg or "429" in error_msg
                is_timeout = "timeout" in error_msg or "timed out" in error_msg
                is_unavailable = "unavailable" in error_msg
                
                if is_unavailable:
                    print(f"  [FALLBACK] {model} unavailable, trying next model...")
                    break
                
                if attempt < MAX_RETRIES - 1 and (is_rate_limit or is_timeout):
                    delay = RETRY_BASE_DELAY * (2 ** attempt)
                    print(f"  [RETRY] {model} attempt {attempt + 1} failed: {e}. Retrying in {delay}s...")
                    time.sleep(delay)
                else:
                    print(f"  [FALLBACK] {model} failed after {attempt + 1} attempts, trying next model...")
                    break
    
    print("  [ERROR] All LLM models failed")
    return None


# ============================================================================
# Gemini ç­›é€‰
# ============================================================================

FILTER_PROMPT = """ä½ æ˜¯ä¸€ä¸ª AI æŠ€æœ¯çƒ­ç‚¹ç­›é€‰åŠ©æ‰‹ã€‚è¯·ä»ä»¥ä¸‹æ–‡ç« åˆ—è¡¨ä¸­ï¼Œç­›é€‰å‡ºä¸ AI/äººå·¥æ™ºèƒ½æœ€ç›¸å…³ã€æœ€æœ‰ä»·å€¼çš„å†…å®¹ã€‚

è¯„åˆ¤æ ‡å‡†ï¼š
1. ä¸ AIã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€å¤§è¯­è¨€æ¨¡å‹ç­‰ç›´æ¥ç›¸å…³
2. å†…å®¹æœ‰ä»·å€¼ï¼ˆæŠ€æœ¯çªç ´ã€é‡è¦å‘å¸ƒã€è¡Œä¸šåŠ¨æ€ç­‰ï¼‰
3. ä¼˜å…ˆé€‰æ‹©çƒ­åº¦é«˜ã€å½±å“åŠ›å¤§çš„å†…å®¹

è¯·ä»ä¸‹é¢çš„æ–‡ç« ä¸­é€‰å‡ºæœ€å¤š {limit} ç¯‡æœ€ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ã€‚

æ–‡ç« åˆ—è¡¨ï¼š
{articles}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼Œ**æ‰€æœ‰æ–‡æœ¬å†…å®¹å¿…é¡»ç¿»è¯‘æˆä¸­æ–‡**ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```json
{{
  "selected": [
    {{
      "index": 0,
      "title_cn": "ä¸­æ–‡æ ‡é¢˜",
      "reason_cn": "ä¸­æ–‡æ¨èç†ç”±ï¼ˆä½œä¸ºè¯¥èµ„è®¯çš„ç®€çŸ­æ€»ç»“ï¼Œ50å­—ä»¥å†…ï¼‰",
      "tags": ["trending", "tech"],
      "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"]
    }}
  ]
}}
```

tags æ ‡ç­¾è¯´æ˜ï¼ˆå¯å¤šé€‰ï¼‰ï¼š
- trending: çƒ­ç‚¹é€Ÿè§ˆ - é€‚åˆäº§å“ç»ç†/åˆ›ä¸šè€…/æ³›ç§‘æŠ€çˆ±å¥½è€…ï¼ˆæ–°äº§å“å‘å¸ƒã€åº”ç”¨åœºæ™¯ã€æœ‰è¶£åŠ¨æ€ï¼‰
- tech: æŠ€æœ¯å‰æ²¿ - é€‚åˆå¼€å‘è€…/æŠ€æœ¯äººå‘˜ï¼ˆå¼€æºé¡¹ç›®ã€æŠ€æœ¯æ•™ç¨‹ã€APIæ›´æ–°ã€æŠ€æœ¯çªç ´ï¼‰
- business: å•†ä¸šæ´å¯Ÿ - é€‚åˆæŠ•èµ„äºº/å•†ä¸šå†³ç­–è€…ï¼ˆèèµ„ã€å¹¶è´­ã€å…¬å¸æˆ˜ç•¥ã€å¸‚åœºåˆ†æï¼‰

keywords: æå– 2-5 ä¸ªæ ¸å¿ƒå…³é”®è¯ï¼ˆå¦‚ï¼šGPT-5ã€OpenAIã€å¤šæ¨¡æ€ã€å¼€æºï¼‰

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚index æ˜¯æ–‡ç« åœ¨åˆ—è¡¨ä¸­çš„ç´¢å¼•ï¼ˆä» 0 å¼€å§‹ï¼‰ã€‚
"""

SUMMARY_PROMPT = """è¯·æ ¹æ®ä»¥ä¸‹ä»Šæ—¥ AI çƒ­ç‚¹æ–°é—»çš„æ ‡é¢˜å’Œæ¨èç†ç”±ï¼Œå†™ä¸€æ®µçº¦ 50 å­—çš„ç®€çŸ­æ—¥æŠ¥ç»¼è¿°ã€‚
ç»¼è¿°åº”ç‚¹å‡ºä»Šå¤©æœ€å€¼å¾—å…³æ³¨çš„ 1-3 ä¸ªæ ¸å¿ƒçƒ­ç‚¹äº‹ä»¶ã€‚

çƒ­ç‚¹åˆ—è¡¨ï¼š
{content}

è¯·ç›´æ¥è¿”å›ç»¼è¿°æ–‡æœ¬ï¼Œä¸è¦åŠ ä»»ä½•å‰ç¼€æˆ–æ ¼å¼ã€‚
"""

GITHUB_TRANSLATE_PROMPT = """è¯·ä¸ºä»¥ä¸‹ GitHub AI é¡¹ç›®ç”Ÿæˆä¸­æ–‡ä»‹ç»ã€‚

é¡¹ç›®åˆ—è¡¨ï¼š
{projects}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼š
```json
{{
  "translated": [
    {{
      "index": 0,
      "description_cn": "é¡¹ç›®ä¸­æ–‡ä»‹ç»ï¼ˆä¸€å¥è¯ï¼Œ50å­—ä»¥å†…ï¼‰",
      "ai_reason": "æ¨èç†ç”±ï¼ˆä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨ï¼Œ30å­—ä»¥å†…ï¼‰"
    }}
  ]
}}
```

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""

HUGGINGFACE_TRANSLATE_PROMPT = """è¯·ä¸ºä»¥ä¸‹ HuggingFace çƒ­é—¨æ¨¡å‹ç”Ÿæˆä¸­æ–‡ä»‹ç»ã€‚

æ¨¡å‹åˆ—è¡¨ï¼š
{models}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼š
```json
{{
  "translated": [
    {{
      "index": 0,
      "description_cn": "æ¨¡å‹ä¸­æ–‡ä»‹ç»ï¼ˆä¸€å¥è¯ï¼Œ50å­—ä»¥å†…ï¼‰",
      "ai_reason": "æ¨èç†ç”±ï¼ˆä¸ºä»€ä¹ˆå€¼å¾—å…³æ³¨ï¼Œ30å­—ä»¥å†…ï¼‰"
    }}
  ]
}}
```

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""


def calculate_similarity(s1: str, s2: str) -> float:
    """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ç›¸ä¼¼åº¦"""
    return difflib.SequenceMatcher(None, s1, s2).ratio()

def deduplicate_items(items: list[dict], threshold: float = 0.6) -> list[dict]:
    """
    å¯¹çƒ­ç‚¹è¿›è¡Œå»é‡å’Œèšåˆ
    
    Args:
        items: çƒ­ç‚¹åˆ—è¡¨
        threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0 - 1.0)
    
    Returns:
        å¤„ç†åçš„çƒ­ç‚¹åˆ—è¡¨ï¼ˆåŒ…å« duplicate_group ä¿¡æ¯ï¼‰
    """
    if not items:
        return []
    
    # æŒ‰æ¥æºåˆ†ç»„ï¼Œé¿å…åŒä¸€æ¥æºå†…éƒ¨å»é‡ï¼ˆå‡è®¾åŒä¸€æ¥æºä¸ä¼šå‘é‡å¤å†…å®¹ï¼‰
    # ä½†è¿™é‡Œæˆ‘ä»¬è¦è·¨æ¥æºå»é‡
    
    # ç»“æœåˆ—è¡¨
    processed_items = []
    
    # è®°å½•å·²å¤„ç†çš„ç´¢å¼•
    processed_indices = set()
    
    for i in range(len(items)):
        if i in processed_indices:
            continue
            
        current_item = items[i]
        current_item["duplicate_group"] = str(uuid.uuid4())
        current_item["is_primary"] = True
        current_item["similarity_score"] = 0
        
        # æŸ¥æ‰¾é‡å¤é¡¹
        duplicates = []
        for j in range(i + 1, len(items)):
            if j in processed_indices:
                continue
                
            compare_item = items[j]
            
            # æ¯”è¾ƒæ ‡é¢˜ç›¸ä¼¼åº¦
            title_sim = calculate_similarity(current_item["title"], compare_item["title"])
            
            # å¦‚æœæ ‡é¢˜ç›¸ä¼¼åº¦é«˜ï¼Œè§†ä¸ºé‡å¤
            if title_sim >= threshold:
                duplicates.append({
                    "index": j,
                    "score": title_sim,
                    "item": compare_item
                })
        
        # å¤„ç†é‡å¤é¡¹
        for dup in duplicates:
            idx = dup["index"]
            processed_indices.add(idx)
            
            dup_item = dup["item"]
            dup_item["duplicate_group"] = current_item["duplicate_group"]
            dup_item["is_primary"] = False
            dup_item["similarity_score"] = dup["score"]
            dup_item["duplicate_of"] = current_item["url"] # é€»è¾‘ä¸ŠæŒ‡å‘ä¸»æ¡ç›®ï¼Œå®é™…å­˜å‚¨æ—¶éœ€è¦å…ˆå­˜ä¸»æ¡ç›®è·å–IDï¼Œæˆ–è€…ä»…ç”¨ group ID å…³è”
            
            processed_items.append(dup_item)
            
            # åˆå¹¶æ ‡ç­¾å’Œå…³é”®è¯
            if "tags" in dup_item:
                current_item["tags"] = list(set(current_item.get("tags", []) + dup_item["tags"]))
            if "keywords" in dup_item:
                current_item["keywords"] = list(set(current_item.get("keywords", []) + dup_item["keywords"]))

        processed_indices.add(i)
        processed_items.append(current_item)
        
    return processed_items

def filter_items_with_gemini(
    client: OpenAI,
    items: list[dict],
    limit: int = MAX_ITEMS_PER_SOURCE
) -> list[dict]:
    """
    ä½¿ç”¨ LLM ç­›é€‰æœ€ç›¸å…³çš„æ–‡ç« ï¼Œå¹¶ç”Ÿæˆä¸­æ–‡æ ‡é¢˜å’Œç†ç”±
    """
    if not items:
        return []
    
    # æ„å»ºæ–‡ç« åˆ—è¡¨æ–‡æœ¬
    articles_text = ""
    for i, item in enumerate(items):
        articles_text += f"\n[{i}] æ ‡é¢˜: {item['title']}\n"
        if item.get("summary"):
            articles_text += f"    æ‘˜è¦: {item['summary'][:200]}\n"
    
    prompt = FILTER_PROMPT.format(limit=limit, articles=articles_text)
    
    response_text = call_llm_with_retry(
        client,
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )
    
    if not response_text:
        return items[:limit]
    
    json_match = re.search(r"\{[\s\S]*\}", response_text)
    if not json_match:
        print("  [WARN] Could not parse LLM response, returning original items")
        return items[:limit]
    
    try:
        result = json.loads(json_match.group())
    except json.JSONDecodeError:
        print("  [WARN] Invalid JSON from LLM, returning original items")
        return items[:limit]
    
    selected_map = {item["index"]: item for item in result.get("selected", [])}
    selected_indices = [item["index"] for item in result.get("selected", [])]
    
    filtered = []
    for idx in selected_indices:
        if 0 <= idx < len(items):
            item = items[idx].copy()
            gemini_data = selected_map.get(idx, {})
            
            if gemini_data.get("title_cn"):
                item["title"] = gemini_data["title_cn"]
            if gemini_data.get("reason_cn"):
                item["ai_reason"] = gemini_data["reason_cn"]
            if gemini_data.get("tags"):
                item["tags"] = gemini_data["tags"]
            if gemini_data.get("keywords"):
                item["keywords"] = gemini_data["keywords"]
            
            filtered.append(item)
    
    return filtered[:limit]


ANALYSIS_PROMPT = """è¯·æ ¹æ®ä»¥ä¸‹ä»Šæ—¥ AI çƒ­ç‚¹æ–°é—»ï¼Œç”Ÿæˆä¸€ä»½æ·±åº¦çš„æ—¥æŠ¥åˆ†æã€‚

çƒ­ç‚¹åˆ—è¡¨ï¼š
{content}

è¯·ä»¥ JSON æ ¼å¼è¿”å›ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
1. focus_events: æŒ‘é€‰ 1-3 ä¸ªæœ€é‡è¦çš„ç„¦ç‚¹äº‹ä»¶ï¼Œè¿›è¡Œæ·±åº¦è§£è¯»ã€‚
2. overview: ä»Šæ—¥æ•´ä½“è¶‹åŠ¿ç»¼è¿°ï¼ˆ100å­—å·¦å³ï¼‰ã€‚
3. keywords: æå–ä»Šæ—¥æ‰€æœ‰èµ„è®¯çš„æ ¸å¿ƒå…³é”®è¯åŠå…¶çƒ­åº¦ï¼ˆå‡ºç°é¢‘ç‡/é‡è¦æ€§ï¼Œ1-10åˆ†ï¼‰ã€‚

JSON æ ¼å¼å¦‚ä¸‹ï¼š
```json
{{
  "focus_events": [
    {{
      "title": "äº‹ä»¶æ ‡é¢˜",
      "summary": "äº‹ä»¶ç®€è¿°",
      "why": "å‘ç”ŸåŸå› /èƒŒæ™¯ï¼ˆä¸ºä»€ä¹ˆé‡è¦ï¼Ÿï¼‰",
      "impact": "åç»­å½±å“/è¡Œä¸šæ„ä¹‰"
    }}
  ],
  "overview": "ä»Šæ—¥ AI é¢†åŸŸæ•´ä½“å‘ˆç°...è¶‹åŠ¿ï¼Œå…¶ä¸­...",
  "keywords": {{
    "å…³é”®è¯1": 10,
    "å…³é”®è¯2": 8,
    "å…³é”®è¯3": 5
  }}
}}
```

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""

def generate_daily_analysis(client: OpenAI, all_selected: dict[str, list[dict]]) -> dict | None:
    """ç”Ÿæˆæ¯æ—¥æ·±åº¦åˆ†æ"""
    content = ""
    for source, items in all_selected.items():
        for item in items:
            title = item.get("title", "")
            reason = item.get("ai_reason", "")
            tags = ",".join(item.get("tags", []))
            content += f"- [{tags}] {title}: {reason}\n"
    
    if not content:
        return None

    prompt = ANALYSIS_PROMPT.format(content=content[:8000]) # å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦
    
    response_text = call_llm_with_retry(
        client,
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )
    
    if not response_text:
        return None
    
    json_match = re.search(r"\{[\s\S]*\}", response_text)
    if not json_match:
        return None
        
    try:
        return json.loads(json_match.group())
    except json.JSONDecodeError:
        return None

def upsert_daily_analysis(supabase: Client, analysis_data: dict) -> bool:
    """ä¿å­˜æ¯æ—¥æ·±åº¦åˆ†æ"""
    if not analysis_data:
        return False
        
    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    
    data = {
        "report_date": today,
        "focus_events": analysis_data.get("focus_events", []),
        "overview": analysis_data.get("overview", ""),
        "keywords": analysis_data.get("keywords", {})
    }
    
    try:
        supabase.table("daily_analysis").upsert(data, on_conflict="report_date").execute()
        return True
    except Exception as e:
        print(f"  [ERROR] Failed to save daily analysis: {e}")
        return False

def generate_daily_summary(client: OpenAI, all_selected: dict[str, list[dict]]) -> str:
    content = ""
    for source, items in all_selected.items():
        for item in items:
            title = item.get("title", "")
            reason = item.get("ai_reason", "")
            content += f"- {title}: {reason}\n"
    
    if not content:
        return "ä»Šæ—¥æš‚æ— é‡ç‚¹èµ„è®¯ã€‚"

    prompt = SUMMARY_PROMPT.format(content=content[:5000])
    
    response_text = call_llm_with_retry(
        client,
        messages=[{"role": "user", "content": prompt}]
    )
    
    return response_text or "ä»Šæ—¥ AI çƒ­ç‚¹èµ„è®¯æ±‡æ€»ã€‚"


def translate_github_trending(client: OpenAI, items: list[dict], limit: int = 20) -> list[dict]:
    if not items:
        return []
    
    items = items[:limit]
    projects_text = ""
    for i, item in enumerate(items):
        projects_text += f"\n[{i}] {item['name']}"
        projects_text += f"\n    â­{item['stars']} | Language: {item.get('language', 'N/A')}"
        if item.get("description"):
            projects_text += f"\n    {item['description'][:200]}"
    
    prompt = GITHUB_TRANSLATE_PROMPT.format(projects=projects_text)
    
    response_text = call_llm_with_retry(
        client,
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )
    
    if not response_text:
        return items
    
    json_match = re.search(r"\{[\s\S]*\}", response_text)
    if not json_match:
        return items
    
    try:
        result = json.loads(json_match.group())
        translated_map = {t["index"]: t for t in result.get("translated", [])}
        
        for i, item in enumerate(items):
            if i in translated_map:
                t = translated_map[i]
                item["description_cn"] = t.get("description_cn", "")
                item["ai_reason"] = t.get("ai_reason", "")
        
        return items
    except (json.JSONDecodeError, KeyError):
        return items


def translate_huggingface_trending(client: OpenAI, items: list[dict], limit: int = 20) -> list[dict]:
    if not items:
        return []
    
    items = items[:limit]
    models_text = ""
    for i, item in enumerate(items):
        models_text += f"\n[{i}] {item['model_id']}"
        models_text += f"\n    ğŸ”¥{item.get('trending_score', 0)} | Task: {item.get('pipeline_tag', 'N/A')}"
        models_text += f"\n    Downloads: {item.get('downloads', 0)} | Likes: {item.get('likes', 0)}"
    
    prompt = HUGGINGFACE_TRANSLATE_PROMPT.format(models=models_text)
    
    response_text = call_llm_with_retry(
        client,
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}
    )
    
    if not response_text:
        return items
    
    json_match = re.search(r"\{[\s\S]*\}", response_text)
    if not json_match:
        return items
    
    try:
        result = json.loads(json_match.group())
        translated_map = {t["index"]: t for t in result.get("translated", [])}
        
        for i, item in enumerate(items):
            if i in translated_map:
                t = translated_map[i]
                item["description_cn"] = t.get("description_cn", "")
                item["ai_reason"] = t.get("ai_reason", "")
        
        return items
    except (json.JSONDecodeError, KeyError):
        return items


# ============================================================================
# æ•°æ®åº“æ“ä½œ
# ============================================================================

def ensure_tables_exist(supabase: Client) -> None:
    """ç¡®ä¿æ‰€éœ€çš„è¡¨å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡æ—¥æŠ¥ä¿å­˜"""
    try:
        # æ£€æŸ¥ daily_reports è¡¨æ˜¯å¦å­˜åœ¨
        supabase.table("daily_reports").select("id").limit(1).execute()
        # print("  daily_reports table: OK")
    except Exception:
        print("  [WARN] daily_reports table not found. Daily report will not be saved.")
        print("  To create it, run this SQL in Supabase Dashboard:")
        print("  CREATE TABLE daily_reports (")
        print("    id UUID DEFAULT gen_random_uuid() PRIMARY KEY,")
        print("    report_date DATE DEFAULT CURRENT_DATE UNIQUE,")
        print("    content TEXT NOT NULL,")
        print("    summary TEXT,")
        print("    created_at TIMESTAMPTZ DEFAULT NOW()")
        print("  );")


def upsert_hotspots(supabase: Client, items: list[dict], source: str) -> int:
    """
    å°†çƒ­ç‚¹æ•°æ®å†™å…¥ Supabase
    """
    if not items:
        return 0
    
    records = []
    for item in items:
        records.append({
            "title": item["title"],
            "url": item["url"],
            "summary": item.get("ai_reason") or item.get("summary", ""), # ä¼˜å…ˆä½¿ç”¨ AI ç”Ÿæˆçš„ä¸­æ–‡ç†ç”±
            "source": source,
            "tags": item.get("tags", []),
            "keywords": item.get("keywords", []),
            "duplicate_group": item.get("duplicate_group"),
            "is_primary": item.get("is_primary", True),
            "similarity_score": item.get("similarity_score", 0),
            "is_published": True,
        })
    
    try:
        # ä½¿ç”¨ upsertï¼Œä»¥ url ä¸ºå†²çªæ£€æµ‹å­—æ®µ
        result = supabase.table("hotspots").upsert(
            records,
            on_conflict="url"
        ).execute()
        
        return len(result.data) if result.data else 0
    
    except Exception as e:
        print(f"  [ERROR] Failed to upsert hotspots: {e}")
        return 0


def save_daily_report(supabase: Client, report_content: str, summary: str = "") -> bool:
    """ä¿å­˜æ¯æ—¥æŠ¥å‘Šåˆ°æ•°æ®åº“ï¼ˆå¢é‡æ¨¡å¼ï¼šè¿½åŠ æ–°æ¥æºï¼Œä¸è¦†ç›–å·²æœ‰å†…å®¹ï¼‰"""
    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    
    try:
        existing = supabase.table("daily_reports").select("content, summary").eq("report_date", today).execute()
        
        # Cast data to a list of dicts to satisfy type checker
        existing_data = cast(list[dict[str, Any]], existing.data)

        if existing_data:
            old_content = str(existing_data[0].get("content", "") or "")
            old_summary = str(existing_data[0].get("summary", "") or "")
            
            existing_sources = set(re.findall(r"^## (.+)$", old_content, re.MULTILINE))
            new_sections = re.split(r"(?=^## )", report_content, flags=re.MULTILINE)
            
            new_content_parts = []
            for section in new_sections:
                match = re.match(r"^## (.+)$", section, re.MULTILINE)
                if match:
                    source_name = match.group(1)
                    if source_name not in existing_sources:
                        new_content_parts.append(section.strip())
            
            if new_content_parts:
                merged_content = old_content.rstrip() + "\n\n" + "\n\n".join(new_content_parts)
            else:
                merged_content = old_content
            
            merged_summary = summary if summary else old_summary
            
            data = {
                "report_date": today,
                "content": merged_content,
                "summary": merged_summary
            }
        else:
            data = {
                "report_date": today,
                "content": report_content,
                "summary": summary
            }
        
        supabase.table("daily_reports").upsert(data, on_conflict="report_date").execute()
        return True
    except Exception as e:
        print(f"[ERROR] Failed to save daily report: {e}")
        return False


def upsert_github_trending(supabase: Client, items: list[dict]) -> int:
    if not items:
        return 0
    
    records = []
    for item in items:
        records.append({
            "name": item["name"],
            "url": item["url"],
            "description": item.get("description", ""),
            "description_cn": item.get("description_cn", ""),
            "stars": item.get("stars", 0),
            "forks": item.get("forks", 0),
            "language": item.get("language", ""),
            "topics": item.get("topics", []),
            "ai_reason": item.get("ai_reason", ""),
            "is_published": True,
        })
    
    try:
        result = supabase.table("github_trending").upsert(
            records, on_conflict="url"
        ).execute()
        return len(result.data) if result.data else 0
    except Exception as e:
        print(f"  [ERROR] Failed to upsert github_trending: {e}")
        return 0


def upsert_huggingface_trending(supabase: Client, items: list[dict]) -> int:
    if not items:
        return 0
    
    records = []
    for item in items:
        records.append({
            "model_id": item["model_id"],
            "url": item["url"],
            "description_cn": item.get("description_cn", ""),
            "likes": item.get("likes", 0),
            "downloads": item.get("downloads", 0),
            "trending_score": item.get("trending_score", 0),
            "pipeline_tag": item.get("pipeline_tag", ""),
            "tags": item.get("tags", []),
            "ai_reason": item.get("ai_reason", ""),
            "is_published": True,
        })
    
    try:
        result = supabase.table("huggingface_trending").upsert(
            records, on_conflict="url"
        ).execute()
        return len(result.data) if result.data else 0
    except Exception as e:
        print(f"  [ERROR] Failed to upsert huggingface_trending: {e}")
        return 0


# ============================================================================
# æŠ¥å‘Šç”Ÿæˆ
# ============================================================================

def generate_daily_report(
    all_selected: dict[str, list[dict]], 
    daily_summary: str = ""
) -> str:
    """ç”Ÿæˆæ¯æ—¥ Markdown æŠ¥å‘Šï¼ˆä¸­æ–‡ç‰ˆï¼‰"""
    today = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    
    lines = [
        f"# AI çƒ­ç‚¹æ—¥æŠ¥ - {today}",
        "",
        f"ç”Ÿæˆæ—¶é—´: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S')} UTC",
        "",
    ]
    
    if daily_summary:
        lines.append(f"> **ä»Šæ—¥ç»¼è¿°**ï¼š{daily_summary}")
        lines.append("")
        lines.append("---")
        lines.append("")
    
    total_count = sum(len(items) for items in all_selected.values())
    lines.append(f"ä»Šæ—¥å…±æ”¶å½• **{total_count}** æ¡çƒ­ç‚¹ï¼Œæ¥è‡ª **{len(all_selected)}** ä¸ªä¿¡æ¯æºã€‚")
    lines.append("")
    
    for source, items in all_selected.items():
        lines.append(f"## {source}")
        lines.append("")
        
        for i, item in enumerate(items):
            title = item["title"] # å·²ç»æ˜¯ä¸­æ–‡
            url = item["url"]
            # ä¼˜å…ˆä½¿ç”¨ AI ç”Ÿæˆçš„ç†ç”±ï¼Œå¦åˆ™ä½¿ç”¨åŸæ–‡æ‘˜è¦
            reason = item.get("ai_reason") or item.get("summary", "")
            # é™åˆ¶æ‘˜è¦é•¿åº¦ï¼Œé¿å…è¿‡é•¿
            if reason and len(reason) > 200:
                reason = reason[:200] + "..."
            
            lines.append(f"### {i+1}. [{title}]({url})")
            if reason:
                lines.append(f"> {reason}")
            lines.append("")
        
        lines.append("")
    
    return "\n".join(lines)


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    print("=" * 60)
    print("AI Hotspot Daily Fetcher")
    print("=" * 60)
    print()
    
    print("[1/8] Initializing...")
    try:
        client = init_llm()
        supabase = init_supabase()
        feeds, trending_config = load_feed_config()
        
        # Allow limiting feeds for testing
        max_feeds = os.environ.get("MAX_FEEDS")
        if max_feeds:
            print(f"[TEST] Limiting to first {max_feeds} feeds")
            feeds = feeds[:int(max_feeds)]
            # Clear trending if testing speed
            trending_config = {}

        print(f"  Loaded {len(feeds)} feed sources + {len(trending_config)} trending sources")
        ensure_tables_exist(supabase)
    except Exception as e:
        print(f"[FATAL] Initialization failed: {e}")
        sys.exit(1)
    
    print()
    print("[2/8] Fetching feeds (RSS + crawlers)...")
    raw_data = fetch_all_feeds(feeds)
    
    if not raw_data:
        print("[WARN] No data fetched from any source")
    
    print()
    print("[3/8] Filtering & Translating feeds with LLM (Parallel)...")
    all_selected = {}

    def process_source_with_llm(source_name: str, source_items: list[dict]) -> tuple[str, list[dict]]:
        """Process a single source with LLM"""
        print(f"  Processing: {source_name} ({len(source_items)} items)")
        try:
            # Re-initialize client for thread safety if needed, or pass it in
            # OpenAI client is thread-safe, but we can also instantiate a new one if we see issues
            local_client = init_llm() 
            selected_items = filter_items_with_gemini(local_client, source_items)
            return source_name, selected_items
        except Exception as e:
            print(f"  [ERROR] Failed to process {source_name}: {e}")
            return source_name, []

    # Use ThreadPoolExecutor for parallel LLM processing
    # Limit workers to avoid Rate Limit errors (429) from API
    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        future_to_source = {
            executor.submit(process_source_with_llm, source, items): source 
            for source, items in raw_data.items()
        }
        
        for future in concurrent.futures.as_completed(future_to_source):
            source = future_to_source[future]
            try:
                name, selected = future.result()
                if selected:
                    all_selected[name] = selected
                    print(f"    Selected {len(selected)} items for {name}")
            except Exception as e:
                print(f"    [ERROR] Exception processing {source}: {e}")

    print()
    print("[3.5/8] Deduplicating items across sources...")
    # Flatten items for deduplication
    flat_items = []
    for source, items in all_selected.items():
        for item in items:
            item["_source_key"] = source
            flat_items.append(item)
            
    if flat_items:
        deduped_items = deduplicate_items(flat_items)
        print(f"  Processed {len(flat_items)} items, found {len([i for i in deduped_items if not i.get('is_primary')])} duplicates")
        
        # Re-group by source
        all_selected = {}
        for item in deduped_items:
            source = item.pop("_source_key")
            if source not in all_selected:
                all_selected[source] = []
            all_selected[source].append(item)
    
    print()
    print("[4/8] Fetching trending data...")
    github_items = []
    huggingface_items = []
    
    if "github" in trending_config:
        print("  Fetching GitHub Trending AI...")
        github_items = fetch_github_trending_ai(limit=30)
        print(f"    Found {len(github_items)} repos")
    
    if "huggingface" in trending_config:
        print("  Fetching HuggingFace Trending...")
        huggingface_items = fetch_huggingface_trending(limit=30)
        print(f"    Found {len(huggingface_items)} models")
    
    print()
    print("[5/8] Translating trending data with LLM...")
    if github_items:
        print("  Translating GitHub items...")
        github_items = translate_github_trending(client, github_items, limit=20)
    if huggingface_items:
        print("  Translating HuggingFace items...")
        huggingface_items = translate_huggingface_trending(client, huggingface_items, limit=20)
    
    print()
    print("[6/8] Generating daily summary & analysis...")
    daily_summary = ""
    daily_analysis = None
    
    if all_selected:
        try:
            # å¹¶è¡Œç”Ÿæˆç»¼è¿°å’Œæ·±åº¦åˆ†æ
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                future_summary = executor.submit(generate_daily_summary, client, all_selected)
                future_analysis = executor.submit(generate_daily_analysis, client, all_selected)
                
                daily_summary = future_summary.result()
                print(f"  Summary: {daily_summary}")
                
                daily_analysis = future_analysis.result()
                if daily_analysis:
                    print(f"  Analysis generated: {len(daily_analysis.get('focus_events', []))} focus events")
        except Exception as e:
            print(f"  [WARN] Skipped summary/analysis generation: {e}")
    
    print()
    print("[7/8] Saving to database...")
    total_saved = 0
    
    for source, items in all_selected.items():
        count = upsert_hotspots(supabase, items, source)
        total_saved += count
        print(f"  {source}: {count} records")
    
    if daily_analysis:
        if upsert_daily_analysis(supabase, daily_analysis):
            print("  Daily analysis saved successfully")
    
    if github_items:
        count = upsert_github_trending(supabase, github_items)
        total_saved += count
        print(f"  GitHub Trending: {count} records")
    
    if huggingface_items:
        count = upsert_huggingface_trending(supabase, huggingface_items)
        total_saved += count
        print(f"  HuggingFace Trending: {count} records")
    
    print(f"  Total saved: {total_saved} records")
    
    print()
    print("[8/8] Generating & Saving daily report...")
    report = generate_daily_report(all_selected, daily_summary)
    
    if save_daily_report(supabase, report, daily_summary):
        print("  Daily report saved successfully")
    else:
        print("  [WARN] Failed to save daily report")
    
    print()
    print("=" * 60)
    print("REPORT PREVIEW")
    print("=" * 60)
    print(report[:2000])
    if len(report) > 2000:
        print("... (truncated)")
    
    print()
    print("=" * 60)
    print("COMPLETED")
    print("=" * 60)


if __name__ == "__main__":
    main()
